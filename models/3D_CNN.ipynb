{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# بسم الله الرحمن الرحيم"
      ],
      "metadata": {
        "id": "RlGiVvxsmCtc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpNEAiUxiwtn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load data"
      ],
      "metadata": {
        "id": "hY0gcfo3YlJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('/content/drive/MyDrive/Data set/Shop DataSet.zip','r') as file :\n",
        "  file.extractall('/content/Data set')"
      ],
      "metadata": {
        "id": "ZMxe-oFOYhXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = {}\n",
        "datapath = '/content/Data set/Shop DataSet'\n",
        "\n",
        "for filename in os.listdir(datapath):\n",
        "  videos_paths = []\n",
        "  for video_name in os.listdir(os.path.join(datapath,filename)):\n",
        "    videos_paths.append(os.path.join(datapath,filename,video_name))\n",
        "  dataset[filename] = videos_paths\n"
      ],
      "metadata": {
        "id": "meTiRZUBZ6Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.bar(dataset.keys(), [len(videos) for videos in dataset.values()])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QhGb5sKDw414"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199f6502"
      },
      "source": [
        "data_list = []\n",
        "for category, videos in dataset.items():\n",
        "    for video_path in videos:\n",
        "        data_list.append({'labels': category, 'Video Path': video_path})\n",
        "\n",
        "df_videos = pd.DataFrame(data_list)\n",
        "df_videos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "791dcc78"
      },
      "source": [
        "# Identify the minority and majority classes\n",
        "class_counts = df_videos['labels'].value_counts()\n",
        "minority_class = class_counts.idxmin()\n",
        "majority_class = class_counts.idxmax()\n",
        "minority_count = class_counts[minority_class]\n",
        "majority_count = class_counts[majority_class]\n",
        "\n",
        "print(f\"Minority class: {minority_class} with {minority_count} samples\")\n",
        "print(f\"Majority class: {majority_class} with {majority_count} samples\")\n",
        "\n",
        "# Separate minority and majority class samples\n",
        "df_minority = df_videos[df_videos['labels'] == minority_class]\n",
        "df_majority = df_videos[df_videos['labels'] == majority_class]\n",
        "\n",
        "# Randomly duplicate minority class samples\n",
        "df_minority_oversampled = df_minority.sample(majority_count, replace=True, random_state=42)\n",
        "\n",
        "# Concatenate the oversampled minority class with the majority class\n",
        "df_oversampled = pd.concat([df_majority, df_minority_oversampled], axis=0)\n",
        "\n",
        "# Shuffle the oversampled dataset\n",
        "df_oversampled = df_oversampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nClass distribution after oversampling:\")\n",
        "print(df_oversampled['labels'].value_counts())\n",
        "\n",
        "oversampled_class_counts = df_oversampled['labels'].value_counts()\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.bar(oversampled_class_counts.index, oversampled_class_counts.values)\n",
        "plt.title('Class Distribution After Oversampling')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df_oversampled['labels'] = le.fit_transform(df_oversampled['labels'])\n",
        "df_oversampled"
      ],
      "metadata": {
        "id": "8t3Zv0uYCn57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def video_generator(df, frames_per_video=32, img_size=(224,224)):\n",
        "    for _, row in df.iterrows():\n",
        "        path = row['Video Path']\n",
        "        label = row['labels']\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        frames = []\n",
        "        while len(frames) < frames_per_video:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.resize(frame, img_size)\n",
        "            frame = frame[:, :, ::-1]            # BGR -> RGB\n",
        "            frame = frame.astype(np.float32) / 255.0\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "        # padding if short\n",
        "        while len(frames) < frames_per_video:\n",
        "            frames.append(np.zeros_like(frames[0], dtype=np.float32))\n",
        "        yield np.array(frames, dtype=np.float32), np.int32(label)\n",
        "\n",
        "# output signature (channels_last): (frames, H, W, C)\n",
        "frames_per_video = 16\n",
        "img_size = (256, 256)\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(frames_per_video, img_size[0], img_size[1], 3), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        ")"
      ],
      "metadata": {
        "id": "HXzmy7mECEFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into train, validation , and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the oversampled DataFrame\n",
        "train_df, test_df = train_test_split(df_oversampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create TensorFlow Datasets from the split DataFrames\n",
        "train_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: video_generator(train_df, frames_per_video, img_size),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "\n",
        "test_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: video_generator(test_df, frames_per_video, img_size),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "\n",
        "\n",
        "# optional augmentation function (tf ops)\n",
        "def augment(frames, label):\n",
        "    # frames: (T,H,W,C)\n",
        "    frames = tf.image.random_flip_left_right(frames)\n",
        "    # you can add random crop, brightness, etc.\n",
        "    return frames, label\n",
        "\n",
        "# Apply augmentation to the training dataset\n",
        "train_ds = train_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Batch and prefetch both datasets\n",
        "train_ds = train_ds.batch(4).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(4).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"Training dataset:\")\n",
        "print(train_ds)\n",
        "print(\"\\nTesting dataset:\")\n",
        "print(test_ds)"
      ],
      "metadata": {
        "id": "uEHtAXe3KIef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build 3D CNN model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(frames_per_video , img_size[0], img_size[1], 3)),\n",
        "    tf.keras.layers.Conv3D(64 , (3,3,3) , strides=1 , padding='same' , activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling3D((1,2,2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Conv3D(128 , (3,3,3) , activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling3D((2,2,2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Conv3D(256 , (3,3,3), activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling3D((2,2,2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.GlobalAveragePooling3D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid') # Changed to 1 output unit for binary classification\n",
        "])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z3hvQtytZPOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "PYpmbHf2gQ4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs = 15 ,\n",
        "    validation_data = test_ds\n",
        ")"
      ],
      "metadata": {
        "id": "l7mLFkkAgfms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss , acc = model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "jDB6Pto2zn3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Loss : {loss}\")\n",
        "print(f\"Accuracy : {acc}\")"
      ],
      "metadata": {
        "id": "UR83__NB0D_3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}